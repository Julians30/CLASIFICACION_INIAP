# -*- coding: utf-8 -*-
"""ML_RICE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-W7rCSJBcKEEBSWoRw8UbQquRTsvMFb

# Importaci√≥n de librerias
"""

from google.colab import files
files.upload();

# Analisis y visualizacion
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score

# Modelos de Aprendizaje automatico multiclase
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""# Importaci√≥n de datos"""

df = pd.read_excel('Granos de Arroz.xlsx')
df.info()

"""# Estad√≠sticos de resumen

**Medidas descriptivas de resumen segmentadas por cada tipo de arroz**
"""

resumen = df.groupby('Class').agg(['mean', 'std', 'median', 'quantile'])

cuartiles = df.groupby('Class').quantile([0.25, 0.75]).unstack(level=1)
cuartiles.columns = [f'{col[0]}_Q{int(col[1]*100)}' for col in cuartiles.columns]

medias = df.groupby('Class').mean().add_suffix('_media')
stds = df.groupby('Class').std().add_suffix('_std')
medianas = df.groupby('Class').median().add_suffix('_mediana')
q1 = df.groupby('Class').quantile(0.25).add_suffix('_Q1')
q3 = df.groupby('Class').quantile(0.75).add_suffix('_Q3')

resumen_final = pd.concat([medias, stds, medianas, q1, q3], axis=1)

ruta_salida = "resumen_estadistico_por_clase.xlsx"
resumen_final.to_excel(ruta_salida)

ruta_salida

"""**Tablas de distribuci√≥n de frecuencias para los tipos de clase de arroz**"""

tabla_frecuencias = pd.DataFrame({
    'Frecuencia absoluta': df['Class'].value_counts(),
    'Frecuencia relativa (%)': df['Class'].value_counts(normalize=True) * 100
})

tabla_frecuencias = tabla_frecuencias.sort_index()

print(tabla_frecuencias)

"""**Matriz de correlaci√≥n entre variables num√©ricas**"""

df_numericas = df.select_dtypes(include=['int64', 'float64'])

matriz_correlacion = df_numericas.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": 1}, annot_kws={"size": 15})
plt.xticks(fontsize=15, rotation=45, ha='right')
plt.yticks(fontsize=15, rotation=0)
plt.title('Matriz de correlaci√≥n entre variables independientes', )
plt.tight_layout()
plt.show()

"""**An√°lisis del factor de inflacion de la varianza**"""

X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Class'], errors='ignore')

X_const = add_constant(X)

vif_data = pd.DataFrame()
vif_data['Variable'] = X_const.columns
vif_data['VIF'] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]

print(vif_data)

X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Class', 'Area', 'MajorAxisLength', 'ConvexArea', 'MinorAxisLength'], errors='ignore')

X_const = add_constant(X)

vif_data = pd.DataFrame()
vif_data['Variable'] = X_const.columns
vif_data['VIF'] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]

print(vif_data)

"""# Transformaci√≥n de valores para el entrenamiento de modelos"""

le = LabelEncoder()

df['Class_codificada'] = le.fit_transform(df['Class'])

clases = dict(zip(le.classes_, le.transform(le.classes_)))
print("Mapa de clases codificadas:", clases)

"""**Entrenamiento y evaluaci√≥n de modelos de aprendizaje**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np

X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Class_codificada'], errors='ignore')
y = df['Class_codificada']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)

modelos = {
    'Logistic Regression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(),
    'Naive Bayes': GaussianNB()
}

print("üìä RESULTADOS SIN PCA:\n")
for nombre, modelo in modelos.items():
    modelo.fit(X_train, y_train)

    y_pred_train = modelo.predict(X_train)
    y_pred_test = modelo.predict(X_test)

    print(f"üîπ Modelo: {nombre}")

    print("üî∏ Entrenamiento:")
    print(f"- Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")
    print(classification_report(y_train, y_pred_train))
    print("Matriz de confusi√≥n:\n", confusion_matrix(y_train, y_pred_train))

    print("üî∏ Prueba:")
    print(f"- Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")
    print(classification_report(y_test, y_pred_test))
    print("Matriz de confusi√≥n:\n", confusion_matrix(y_test, y_pred_test))

    print("="*60)

"""# Entrenamiendo de los modelos a traves de la metodolog√≠a de validaci√≥n cruzada"""

print("üìä VALIDACI√ìN CRUZADA (5-FOLD):\n")
for nombre, modelo in modelos.items():
    scores = cross_val_score(modelo, X_scaled, y, cv=5, scoring='accuracy')
    print(f"üîπ Modelo: {nombre}")
    print(f"- Accuracy promedio: {scores.mean():.4f}")
    print(f"- Desviaci√≥n est√°ndar: {scores.std():.4f}")
    print("="*60)

"""# Grid Search para optimizacion de parametros"""

from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier

param_grids = {

    # Logistic Regression
    'Logistic Regression': {
        'C': [1.0, 0.1, 10],
        'fit_intercept': [True, False],
        'copy_X': [True, False],
        'n_jobs': [None],
        'positive': [False]
    },

    # Support Vector Classifier
    'SVM': {
        'C': [1.0, 0.1, 10],
        'kernel': ['rbf', 'linear'],
        'degree': [3, 4],
        'gamma': ['scale', 'auto'],
        'coef0': [0.0, 0.5],
        'shrinking': [True, False],
        'tol': [1e-3, 1e-4],
        'cache_size': [200],
        'class_weight': [None, 'balanced'],
        'max_iter': [-1],
        'decision_function_shape': ['ovr']
    },

    # Random Forest
    'Random Forest': {
        'n_estimators': [100, 200],
        'criterion': ['gini', 'entropy'],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'min_weight_fraction_leaf': [0.0],
        'max_features': ['sqrt', 'log2'],
        'max_leaf_nodes': [None],
        'min_impurity_decrease': [0.0]
    },

    # Artificial Neural Network (MLP)
    'ANN': {
        'hidden_layer_sizes': [(100,), (50, 50)],
        'activation': ['relu', 'tanh'],
        'solver': ['adam'],
        'alpha': [0.0001, 0.001],
        'learning_rate_init': [0.01, 0.001],
        'max_iter': [200, 500]
    },

    # K-Nearest Neighbors
    'KNN': {
        'n_neighbors': [5, 3, 7, 9],
        'weights': ['uniform', 'distance'],
        'leaf_size': [30, 50],
        'metric': ['minkowski'],
        'p': [2]
    }
}

grid_search_results = {}

for nombre, modelo in modelos.items():

    # ANN debe mapearse a MLPClassifier
    if nombre == 'ANN':
        modelo = MLPClassifier(random_state=42)

    grid = GridSearchCV(
        estimator=modelo,
        param_grid=param_grids[nombre],
        scoring='accuracy',
        cv=5,
        n_jobs=-1
    )

    grid.fit(X_train, y_train)

    grid_search_results[nombre] = {
        'best_params': grid.best_params_,
        'best_score': grid.best_score_
    }

for modelo, resultado in grid_search_results.items():
    print(f"\n{modelo}")
    print("Mejores hiperpar√°metros:", resultado['best_params'])
    print("Mejor score CV:", resultado['best_score'])

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Class_codificada'], errors='ignore')
y = df['Class_codificada']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

y_train_pred = mlp.predict(X_train)
y_test_pred = mlp.predict(X_test)

print("üìä RED NEURONAL SIN PCA")
print("\nüîπ Entrenamiento:")
print("Accuracy:", accuracy_score(y_train, y_train_pred))
print(classification_report(y_train, y_train_pred))
print("Matriz de Confusi√≥n:\n", confusion_matrix(y_train, y_train_pred))

print("\nüîπ Prueba:")
print("Accuracy:", accuracy_score(y_test, y_test_pred))
print(classification_report(y_test, y_test_pred))
print("Matriz de Confusi√≥n:\n", confusion_matrix(y_test, y_test_pred))

"""# Curva de aprendizaje del mejor modelo (SVC)"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import learning_curve

# Definici√≥n del modelo SVC
svc_model = SVC()

# C√°lculo de la curva de aprendizaje
train_sizes, train_scores, val_scores = learning_curve(
    estimator=svc_model,
    X=X_scaled,
    y=y,
    cv=5,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 10),
    n_jobs=-1
)

# Promedios y desviaciones est√°ndar
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Gr√°fica
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, marker='o', label='Training accuracy')
plt.plot(train_sizes, val_mean, marker='s', label='Validation accuracy')

plt.fill_between(train_sizes,
                 train_mean - train_std,
                 train_mean + train_std,
                 alpha=0.2)

plt.fill_between(train_sizes,
                 val_mean - val_std,
                 val_mean + val_std,
                 alpha=0.2)

plt.xlabel('Training set size')
plt.ylabel('Accuracy')
plt.title('Learning Curve ‚Äì Support Vector Classifier')
plt.legend()
plt.grid(True)
plt.show()

"""# Elaboraci√≥n del gr√°fico de curva ROC"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier


X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Class_codificada'], errors='ignore')
y = df['Class_codificada']


scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, stratify=y, random_state=42
)


clases = sorted(y.unique())
y_test_bin = label_binarize(y_test, classes=clases)
n_classes = y_test_bin.shape[1]


modelos = {
    'Logistic Regression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True),
    # 'Naive Bayes': GaussianNB(),
    'Red Neuronal': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)
}


plt.figure(figsize=(12, 8))

for nombre, modelo in modelos.items():
    modelo.fit(X_train, y_train)

    if hasattr(modelo, "predict_proba"):
        y_score = modelo.predict_proba(X_test)
    elif hasattr(modelo, "decision_function"):
        y_score = modelo.decision_function(X_test)
        if y_score.ndim == 1:
            y_score = np.expand_dims(y_score, axis=1)
    else:
        print(f"‚ö†Ô∏è {nombre} no soporta probabilidades.")
        continue


    if y_score.shape[1] != n_classes:
        print(f"‚ö†Ô∏è {nombre}: n√∫mero de clases en y_score incorrecto ({y_score.shape[1]} vs {n_classes}). Se omite.")
        continue


    fpr, tpr = dict(), dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])


    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
    mean_tpr /= n_classes

    roc_auc = auc(all_fpr, mean_tpr)
    plt.plot(all_fpr, mean_tpr, label=f"{nombre} (AUC = {roc_auc:.2f})")


plt.plot([0, 1], [0, 1], 'k--', lw=1)
plt.xlabel("Tasa de Falsos Positivos (FPR)", fontsize=20)
plt.ylabel("Tasa de Verdaderos Positivos (TPR)", fontsize=20)
plt.title("Curvas ROC (Promedio Macro) - Clasificaci√≥n Multiclase",fontsize=20)
plt.legend(loc="lower right", fontsize=20)
plt.grid(True)
plt.tight_layout()
plt.show()

# ‚ñ∂Ô∏è 7. Gr√°fico final
plt.plot([0, 1], [0, 1], 'k--', lw=1)
plt.xlabel("Tasa de Falsos Positivos (FPR)")
plt.ylabel("Tasa de Verdaderos Positivos (TPR)")
plt.title("Curvas ROC (Promedio Macro) - Clasificaci√≥n Multiclase",fontsize=15)
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Reducci√≥n de dimensionalidad a traves de An√°lisis de Componentes Principales (ACP)"""

features = df.drop("Class").columns

X = df[features].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

explained = pca.explained_variance_ratio_
cumexp = explained.cumsum()

print("Varianza explicada por componente:")
for i, v in enumerate(explained, 1):
    print(f"PC{i}: {v:.4f}")

print("\nVarianza explicada acumulada:")
for i, v in enumerate(cumexp, 1):
    print(f"PC1‚ÄìPC{i}: {v:.4f}")

# Scree plot y acumulada
plt.figure(figsize=(7,4))
plt.plot(range(1, len(features)+1), explained, marker='o')
plt.title("Scree plot (varianza explicada por PC)")
plt.xlabel("Componente principal")
plt.ylabel("Varianza explicada")
plt.grid(True)
plt.show()

plt.figure(figsize=(7,4))
plt.plot(range(1, len(features)+1), cumexp, marker='o')
plt.title("Varianza explicada acumulada")
plt.xlabel("N√∫mero de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.grid(True)
plt.show()

# DataFrame con PCs y n√∫mero de componentes para ‚â•90% de varianza
df_pca = pd.DataFrame(X_pca, columns=[f"PC{i}" for i in range(1, len(features)+1)])
df_pca["target"] = df["variedad"]

umbral = 0.90
n_opt = int(np.searchsorted(cumexp, umbral) + 1)
print(f"\nN√∫mero de componentes para ‚â•{int(umbral*100)}% de varianza: {n_opt}")
X_pca_reducido = X_pca[:, :n_opt]

# DataFrame con PCs y n√∫mero de componentes para ‚â•90% de varianza
df_pca = pd.DataFrame(X_pca, columns=[f"PC{i}" for i in range(1, len(features)+1)])
df_pca["target"] = df["variedad"]

umbral = 0.90
n_opt = int(np.searchsorted(cumexp, umbral) + 1)
print(f"\nN√∫mero de componentes para ‚â•{int(umbral*100)}% de varianza: {n_opt}")
X_pca_reducido = X_pca[:, :n_opt]
